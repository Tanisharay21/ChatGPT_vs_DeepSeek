{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f64d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99d174f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        28.533700\n",
       "std         14.090348\n",
       "min          5.000000\n",
       "25%         17.000000\n",
       "50%         27.000000\n",
       "75%         38.000000\n",
       "max         60.000000\n",
       "Name: Session_Duration_sec, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(r\"D:\\data_analytics\\projects\\chat gpt vs deepseek\\deepseek_vs_chatgpt.csv\")\n",
    "\n",
    "df_raw = df.copy()\n",
    "df[\"Session_Duration_sec\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1277a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e82e7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Month_Num', 'Weekday', 'AI_Platform', 'AI_Model_Version',\n",
       "       'Active_Users', 'New_Users', 'Churned_Users', 'Daily_Churn_Rate',\n",
       "       'Retention_Rate', 'User_ID', 'Query_Type', 'Input_Text',\n",
       "       'Input_Text_Length', 'Response_Tokens', 'Topic_Category', 'User_Rating',\n",
       "       'User_Experience_Score', 'Session_Duration_sec', 'Device_Type',\n",
       "       'Language', 'Response_Accuracy', 'Response_Speed_sec',\n",
       "       'Response_Time_Category', 'Correction_Needed', 'User_Return_Frequency',\n",
       "       'Customer_Support_Interactions', 'Region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219d0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response_Accuracy                379\n",
       "Date                               0\n",
       "Weekday                            0\n",
       "Month_Num                          0\n",
       "AI_Model_Version                   0\n",
       "Active_Users                       0\n",
       "New_Users                          0\n",
       "Churned_Users                      0\n",
       "Daily_Churn_Rate                   0\n",
       "Retention_Rate                     0\n",
       "User_ID                            0\n",
       "AI_Platform                        0\n",
       "Query_Type                         0\n",
       "Input_Text                         0\n",
       "Response_Tokens                    0\n",
       "Input_Text_Length                  0\n",
       "User_Rating                        0\n",
       "User_Experience_Score              0\n",
       "Session_Duration_sec               0\n",
       "Topic_Category                     0\n",
       "Device_Type                        0\n",
       "Language                           0\n",
       "Response_Speed_sec                 0\n",
       "Response_Time_Category             0\n",
       "Correction_Needed                  0\n",
       "User_Return_Frequency              0\n",
       "Customer_Support_Interactions      0\n",
       "Region                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8924ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 28 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Date                           10000 non-null  object \n",
      " 1   Month_Num                      10000 non-null  int64  \n",
      " 2   Weekday                        10000 non-null  object \n",
      " 3   AI_Platform                    10000 non-null  object \n",
      " 4   AI_Model_Version               10000 non-null  object \n",
      " 5   Active_Users                   10000 non-null  int64  \n",
      " 6   New_Users                      10000 non-null  int64  \n",
      " 7   Churned_Users                  10000 non-null  int64  \n",
      " 8   Daily_Churn_Rate               10000 non-null  float64\n",
      " 9   Retention_Rate                 10000 non-null  float64\n",
      " 10  User_ID                        10000 non-null  object \n",
      " 11  Query_Type                     10000 non-null  object \n",
      " 12  Input_Text                     10000 non-null  object \n",
      " 13  Input_Text_Length              10000 non-null  int64  \n",
      " 14  Response_Tokens                10000 non-null  int64  \n",
      " 15  Topic_Category                 10000 non-null  object \n",
      " 16  User_Rating                    10000 non-null  int64  \n",
      " 17  User_Experience_Score          10000 non-null  float64\n",
      " 18  Session_Duration_sec           10000 non-null  int64  \n",
      " 19  Device_Type                    10000 non-null  object \n",
      " 20  Language                       10000 non-null  object \n",
      " 21  Response_Accuracy              9621 non-null   float64\n",
      " 22  Response_Speed_sec             10000 non-null  float64\n",
      " 23  Response_Time_Category         10000 non-null  object \n",
      " 24  Correction_Needed              10000 non-null  int64  \n",
      " 25  User_Return_Frequency          10000 non-null  int64  \n",
      " 26  Customer_Support_Interactions  10000 non-null  int64  \n",
      " 27  Region                         10000 non-null  object \n",
      "dtypes: float64(5), int64(11), object(12)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc576c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize column names \n",
    "df.columns = df.columns.str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d094dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Series name: date\n",
      "Non-Null Count  Dtype         \n",
      "--------------  -----         \n",
      "10000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1)\n",
      "memory usage: 78.3 KB\n"
     ]
    }
   ],
   "source": [
    "# convert date\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df[\"date\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146f8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop obviously broken rows\n",
    "df = df.dropna(subset=['date', 'ai_platform'])      # drop only those rows where date and ai_platform in missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0377ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Datatypes\n",
    "\n",
    "int_cols = [\n",
    "    'active_users','new_users','churned_users',\n",
    "    'response_tokens','input_text_length',\n",
    "    'session_duration_sec','customer_support_interactions'\n",
    "]\n",
    "\n",
    "float_cols = [\n",
    "    'daily_churn_rate','retention_rate',\n",
    "    'user_rating','user_experience_score',\n",
    "    'response_accuracy','response_speed_sec'\n",
    "]\n",
    "\n",
    "df[int_cols] = df[int_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df[float_cols] = df[float_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d00970f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2024    6607\n",
       "2023    2873\n",
       "2025     520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date\"].dt.year.value_counts()  # 3 years of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['input_text'])   # not required for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e805356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "platform\n",
       "chatgpt     5076\n",
       "deepseek    4924\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "df[\"platform\"] = df[\"ai_platform\"].str.lower() \n",
    "df[\"platform\"].sample(5)\n",
    "df[\"platform\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f5bd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        28.533700\n",
       "std         14.090348\n",
       "min          5.000000\n",
       "25%         17.000000\n",
       "50%         27.000000\n",
       "75%         38.000000\n",
       "max         60.000000\n",
       "Name: session_duration_sec, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Engagement Bucket\n",
    "\n",
    "df[\"session_duration_sec\"].describe()   # the time stamps are not in seconds so we will do that\n",
    "\n",
    "# this is how it looked like before the conversion:\n",
    "\n",
    "# mean        28.533700\n",
    "# std         14.090348\n",
    "# min          5.000000\n",
    "# 25%         17.000000\n",
    "# 50%         27.000000\n",
    "# 75%         38.000000\n",
    "# max         60.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23      1920\n",
       "8015     720\n",
       "8235    2940\n",
       "7575    1440\n",
       "1829    1560\n",
       "139     2340\n",
       "7197    3420\n",
       "9585    2280\n",
       "2324    2340\n",
       "7054    2580\n",
       "Name: session_duration_sec, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"session_duration_sec\"] = df[\"session_duration_sec\"] * 60    # converted to seconds (only_once)\n",
    "df[\"session_duration_sec\"].describe()\n",
    "\n",
    "df[\"session_duration_sec\"].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efb1627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engagement_level\n",
       "High        4220\n",
       "Medium      3676\n",
       "Low         1970\n",
       "Very Low     134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['engagement_level'] = pd.cut(\n",
    "    df['session_duration_sec'],\n",
    "    bins=[0, 300, 900, 1800, 99999],\n",
    "    labels=['Very Low','Low','Medium','High'],\n",
    "    include_lowest=True\n",
    ")\n",
    "df[\"engagement_level\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d7d7a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_duration_sec</th>\n",
       "      <th>response_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>session_duration_sec</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response_tokens</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      session_duration_sec  response_tokens\n",
       "session_duration_sec                1.0000           0.0099\n",
       "response_tokens                     0.0099           1.0000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the result looks like this: High > Medium > Low > Very Low\n",
    "# this arrises the question that is the engagemnet real, or is it session definition bias?\n",
    "\n",
    "# some system defines \"session\" as the time between the first an dthe last interaction, if user leave sthe tab open the session extends.\n",
    "\n",
    "# to check the real engagemnet we are finding the corelation between the \"session time\" and \"response tocken\"\n",
    "# if 'session_duration_sec' and 'response_tokens' both are high, thats the real engagemnet \n",
    "\n",
    "df[['session_duration_sec', 'response_tokens']].corr()\n",
    "\n",
    "# and it shows that the users are not actively interacting with the AI for most of that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a9c921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean      1712.022000\n",
       "std        845.420904\n",
       "min        300.000000\n",
       "25%       1020.000000\n",
       "50%       1620.000000\n",
       "75%       2280.000000\n",
       "max       3600.000000\n",
       "Name: session_duration_sec, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"session_duration_sec\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c475277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean       274.765100\n",
       "std        130.077225\n",
       "min         50.000000\n",
       "25%        162.000000\n",
       "50%        276.000000\n",
       "75%        386.250000\n",
       "max        500.000000\n",
       "Name: response_tokens, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['response_tokens'].describe() \n",
    "\n",
    "# the maximum session time is 60 minutes and the maximum response tocken is 500 which seems a little off because\n",
    "\n",
    "# In real usage, some people might spend more than an hour in a session or receive very long responses.\n",
    "# In this dataset, those cases are grouped at the maximum value instead of being recorded precisely, as in this data the session has stopped exactly at 60 min\n",
    "# So the data is good for comparison and trend analysis, but not for measuring extreme behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "444c3151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_length_category\n",
       "Very Long    4220\n",
       "Long         3676\n",
       "Medium       1970\n",
       "Short         134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['session_length_category'] = pd.cut(\n",
    "    df['session_duration_sec'],\n",
    "    bins=[0, 300, 900, 1800, 99999],\n",
    "    labels=['Short','Medium','Long','Very Long'],\n",
    "    include_lowest=True\n",
    ")\n",
    "df['session_length_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2910d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token_engagement\n",
       "Moderate     3277\n",
       "High         2280\n",
       "Low          2257\n",
       "Very High    2186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_engagement'] = pd.cut(\n",
    "    df['response_tokens'],\n",
    "    bins=[0, 150, 300, 400, 500],\n",
    "    labels=['Low','Moderate','High','Very High'],\n",
    "    include_lowest=True\n",
    ")\n",
    "df['token_engagement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERALL AI PERFORMANCE AND USAGE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec9f411b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>response_accuracy</th>\n",
       "      <th>correction_needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algorithm Explanation</th>\n",
       "      <td>4.8</td>\n",
       "      <td>89.9%</td>\n",
       "      <td>13.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code Optimization</th>\n",
       "      <td>4.8</td>\n",
       "      <td>90.0%</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Debugging</th>\n",
       "      <td>4.8</td>\n",
       "      <td>90.0%</td>\n",
       "      <td>13.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Implementation Help</th>\n",
       "      <td>4.8</td>\n",
       "      <td>89.9%</td>\n",
       "      <td>15.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Content Creation</th>\n",
       "      <td>4.0</td>\n",
       "      <td>80.3%</td>\n",
       "      <td>15.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Practices</th>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>14.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>4.0</td>\n",
       "      <td>80.3%</td>\n",
       "      <td>14.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional Writing</th>\n",
       "      <td>4.0</td>\n",
       "      <td>80.5%</td>\n",
       "      <td>13.6%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_rating response_accuracy correction_needed\n",
       "topic_category                                                        \n",
       "Algorithm Explanation          4.8             89.9%             13.7%\n",
       "Code Optimization              4.8             90.0%             14.1%\n",
       "Debugging                      4.8             90.0%             13.7%\n",
       "Implementation Help            4.8             89.9%             15.9%\n",
       "Content Creation               4.0             80.3%             15.6%\n",
       "Best Practices                 4.0             80.0%             14.9%\n",
       "Education                      4.0             80.3%             14.2%\n",
       "Professional Writing           4.0             80.5%             13.6%"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI impact by user background                       \n",
    "\n",
    "df.head().T\n",
    "\n",
    "ai_impact = df.groupby(\"topic_category\").agg({\n",
    "    \"user_rating\" : \"mean\",\n",
    "    \"response_accuracy\" : \"mean\",\n",
    "    \"response_accuracy\" : \"mean\",\n",
    "    \"correction_needed\" : \"mean\"\n",
    "    }).sort_values(\"user_rating\", ascending = False)\n",
    "\n",
    "ai_impact[\"user_rating\"] = ai_impact[\"user_rating\"].round(1)\n",
    "ai_impact[\"response_accuracy\"] = (ai_impact[\"response_accuracy\"]*100).round(1).map(lambda x: f\"{x}%\")\n",
    "ai_impact[\"correction_needed\"] = (ai_impact[\"correction_needed\"]*100).round(1).map(lambda x: f\"{x}%\")\n",
    "\n",
    "ai_impact\n",
    "\n",
    "# (THE LAMBDA PART HERE IS CONVERTING THE NUMBERS IN STRING, WHILE DOING THE ANALYSIS WE SHOULD CONVERT IT BACK TO NUMERIC)\n",
    "\n",
    "# AI performs significantly better on technical problem-solving tasks such as algorithm explanation, debugging, and code optimization, \n",
    "# where user ratings average around 4.8 and response accuracy is close to 90%. In contrast, content-oriented and advisory topics like content \n",
    "# creation, professional writing, and best practices show lower user satisfaction (around 4.0) and reduced accuracy (~80%), indicating that \n",
    "# AI currently handles structured, objective tasks more effectively than subjective or guideline-based queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8582bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_accuracy</th>\n",
       "      <th>user_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>85.3%</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>85.1%</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>85.1%</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>84.9%</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>84.8%</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         response_accuracy  user_rating\n",
       "language                               \n",
       "German               85.3%          4.4\n",
       "French               85.1%          4.4\n",
       "Spanish              85.1%          4.4\n",
       "English              84.9%          4.4\n",
       "Chinese              84.8%          4.4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Language & region impact\n",
    "\n",
    "language_region_impact = df.groupby(\"language\").agg({\n",
    "    \"response_accuracy\" : \"mean\",\n",
    "    \"user_rating\" : \"mean\"\n",
    "}).sort_values(\"response_accuracy\", ascending = False)\n",
    "\n",
    "language_region_impact[\"user_rating\"] = language_region_impact[\"user_rating\"].round(1)\n",
    "language_region_impact[\"response_accuracy\"] = (language_region_impact[\"response_accuracy\"]*100).round(1).map(lambda x: f\"{x}%\" )\n",
    "\n",
    "# changing the language column in proper language\n",
    "#==============================================================================\n",
    "language_map = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish',\n",
    "    'fr': 'French',\n",
    "    'de': 'German',\n",
    "    'zh': 'Chinese'\n",
    "}\n",
    "\n",
    "df[\"language\"] = df[\"language\"].map(language_map).fillna(df[\"language\"])\n",
    "# ==============================================================================\n",
    "language_region_impact\n",
    "\n",
    "# The AI works equally well across languages. English does not have a noticeable advantage — accuracy stays almost the same regardless of language.\n",
    "# What matters more is how clear and structured the question is, not the language used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1addab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_duration_sec</th>\n",
       "      <th>response_tokens</th>\n",
       "      <th>user_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Laptop/Desktop</th>\n",
       "      <td>28.3 min</td>\n",
       "      <td>273</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mobile</th>\n",
       "      <td>28.6 min</td>\n",
       "      <td>276</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smart Speaker</th>\n",
       "      <td>28.1 min</td>\n",
       "      <td>265</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tablet</th>\n",
       "      <td>29.1 min</td>\n",
       "      <td>272</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_duration_sec  response_tokens  user_rating\n",
       "device_type                                                      \n",
       "Laptop/Desktop             28.3 min              273         4.38\n",
       "Mobile                     28.6 min              276         4.40\n",
       "Smart Speaker              28.1 min              265         4.36\n",
       "Tablet                     29.1 min              272         4.41"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device-based help (mobile vs desktop)\n",
    "\n",
    "Device_based  = df.groupby(\"device_type\").agg({\n",
    "    \"session_duration_sec\" : \"mean\",\n",
    "    \"response_tokens\" : \"mean\",\n",
    "    \"user_rating\" : \"mean\"\n",
    "})\n",
    "\n",
    "Device_based[\"user_rating\"] = Device_based[\"user_rating\"].round(2)\n",
    "Device_based[\"session_duration_sec\"] = (Device_based[\"session_duration_sec\"]/60).round(1).astype(str) + \" min\"\n",
    "Device_based[\"response_tokens\"] = Device_based[\"response_tokens\"].astype(int)\n",
    "\n",
    "Device_based\n",
    "\n",
    "# Session duration remains consistently around 28–29 minutes across all device types, indicating that user engagement with AI is stable \n",
    "# regardless of access medium. Response length and user ratings also show minimal variation, suggesting a device-agnostic user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1066c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major performance comparison: DeepSeek vs ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4613aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_rating</th>\n",
       "      <th>response_speed_sec</th>\n",
       "      <th>retention_rate</th>\n",
       "      <th>response_accuracy</th>\n",
       "      <th>correction_needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai_platform</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChatGPT</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.4 sec</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>80.26%</td>\n",
       "      <td>14.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek</th>\n",
       "      <td>4.8</td>\n",
       "      <td>1.2 sec</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>89.97%</td>\n",
       "      <td>14.34%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_rating response_speed_sec retention_rate response_accuracy  \\\n",
       "ai_platform                                                                    \n",
       "ChatGPT              4.0            3.4 sec          95.0%            80.26%   \n",
       "DeepSeek             4.8            1.2 sec          95.0%            89.97%   \n",
       "\n",
       "            correction_needed  \n",
       "ai_platform                    \n",
       "ChatGPT                14.58%  \n",
       "DeepSeek               14.34%  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# core comparision metrics\n",
    "\n",
    "platform_performance = df.groupby(\"ai_platform\").agg({\n",
    "    \"user_rating\" : \"mean\",\n",
    "    \"response_speed_sec\" : \"mean\",\n",
    "    \"retention_rate\" : \"mean\",\n",
    "    \"response_accuracy\" : \"mean\",\n",
    "    \"correction_needed\" : \"mean\",\n",
    "})\n",
    "\n",
    "platform_performance[\"user_rating\"] = platform_performance[\"user_rating\"].round(2)\n",
    "platform_performance[\"response_speed_sec\"] = platform_performance[\"response_speed_sec\"].round(1).astype(str) + \" sec\"\n",
    "platform_performance[\"response_accuracy\"] = (platform_performance[\"response_accuracy\"]*100).round(2).astype(str) + \"%\"\n",
    "platform_performance[\"retention_rate\"] = (platform_performance[\"retention_rate\"]*100).astype(str) + \"%\"\n",
    "platform_performance[\"correction_needed\"] = (platform_performance[\"correction_needed\"]*100).round(2).astype(str) + \"%\"\n",
    "\n",
    "platform_performance\n",
    "\n",
    "# DeepSeek demonstrates superior performance in speed, accuracy, and user satisfaction, indicating a stronger response quality experience.\n",
    "# However, identical retention rates suggest that platform loyalty is influenced by factors beyond raw performance, such as familiarity, \n",
    "# or use-case dependency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab6810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai_platform</th>\n",
       "      <th>query_type</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>correction_needed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>General</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>General</td>\n",
       "      <td>4.8</td>\n",
       "      <td>15.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>Technical</td>\n",
       "      <td>4.8</td>\n",
       "      <td>13.81%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ai_platform query_type  user_rating correction_needed\n",
       "0     ChatGPT    General          4.0            14.58%\n",
       "1    DeepSeek    General          4.8            15.89%\n",
       "2    DeepSeek  Technical          4.8            13.81%"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Platform × Topic matrix\n",
    "\n",
    "query_performance = df.groupby(['ai_platform','query_type']).agg({\n",
    "    'user_rating':'mean',\n",
    "    'correction_needed':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "query_performance[\"user_rating\"] = query_performance[\"user_rating\"].round(2)\n",
    "query_performance[\"correction_needed\"] = (query_performance[\"correction_needed\"]*100).round(2).astype(str) + \"%\"\n",
    "\n",
    "query_performance\n",
    "\n",
    "# DeepSeek performs best on technical queries, where users rate it highly and also need fewer corrections. This shows that its answers are not \n",
    "# only liked, but are also more accurate and usable.\n",
    "# For general queries, users still rate DeepSeek highly, but they make more edits. This suggests that people enjoy the responses, but often tweak \n",
    "# them for tone, clarity, or personal preference rather than fixing mistakes.\n",
    "\n",
    "# Technical query data for ChatGPT was not present in the dataset, limiting direct cross-platform comparison for this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df[[\n",
    "    # Time\n",
    "    'date', 'weekday', 'month_num',\n",
    "\n",
    "    # Platform & model\n",
    "    'ai_platform', 'ai_model_version',\n",
    "\n",
    "    # User metrics\n",
    "    'active_users', 'new_users', 'churned_users',\n",
    "    'daily_churn_rate', 'retention_rate',\n",
    "\n",
    "    # Engagement\n",
    "    'session_duration_sec',\n",
    "    'session_length_category',\n",
    "    'response_tokens',\n",
    "    'token_engagement',\n",
    "\n",
    "    # Performance\n",
    "    'response_speed_sec',\n",
    "    'response_accuracy',\n",
    "    'response_time_category',\n",
    "    'correction_needed',\n",
    "\n",
    "    # Experience\n",
    "    'user_rating',\n",
    "    'user_experience_score',\n",
    "    'user_return_frequency',\n",
    "\n",
    "    # Context\n",
    "    'query_type',\n",
    "    'topic_category',\n",
    "    'device_type',\n",
    "    'language',\n",
    "    'region'\n",
    "]]\n",
    "\n",
    "#final_df.to_csv(\"ai_platform_sessions_flat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa339c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
